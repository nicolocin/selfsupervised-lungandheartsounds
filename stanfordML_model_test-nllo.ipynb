{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af702c92-009a-4b49-9ec4-b6aa98289803",
   "metadata": {},
   "source": [
    "# Contrastive learning of heart and lung sounds for label-efficient diagnosis\n",
    "\n",
    "An attempt to replicate the heart sound results from the paper with their open-source code\n",
    "\n",
    "- paper: https://www.sciencedirect.com/science/article/pii/S2666389921002671\n",
    "- code: https://github.com/stanfordmlgroup/selfsupervised-lungandheartsounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308feb57-66f5-4358-97a7-1c3aa7a887da",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Run `python process.py [--data \"data directory\"] [--labels_only \"if True, doesn't process only makes labels\"]`\n",
    "\n",
    "\n",
    "\"Provides functionality to take raw audio and text files and generate a processed dir (path/processed) with audio files \n",
    "spliced by respiratory cycle. Also generates disease_labels.csv (mapped disease diagnoses) and symptoms_labels.csv (presence of crackles/wheezes for each respiratory file)\"\n",
    "\n",
    "- `utils/process_heart.py` to process physionet2016 heart sound data\n",
    "- `utils/process_heartchallenge.py` to process PASCAL heart murmur data\n",
    "- `utils/process_lung.py` to process ICBHI2017 lung sound data\n",
    "\n",
    "\n",
    "NOTE: physionet2016 heartsound audio wav data are stored in `nllo/selfsupervised-lungandheartsounds/heart/audio_loc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3740da-af1d-4ada-b1a2-59dd535415e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gpu-srv/PythonProject/pytorch-venv/nllo/selfsupervised-lungandheartsounds\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c4197-7342-46da-ae36-0a08a0cae4c2",
   "metadata": {},
   "source": [
    "- `python process_heart.py` \n",
    "    - to split data into pretain, test and validation sets\n",
    "    - pad audio data such that all wav files are of uniform length (57s, max length of audio)\n",
    " \n",
    "\n",
    "### Step 2\n",
    "\n",
    "- `python data.py`\n",
    "    - generates `.h5` files for each dataset\n",
    " \n",
    "\n",
    "### Step 3\n",
    "\n",
    "- `python contrastive.py --mode pretrain --task heart --log_dir 8_25/spec-pre-large --data ../heart --augment spec --train_prop 1.0 --epoch 10`\n",
    "    - contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8083ea0a-311a-48ca-86f7-c0be27b4dc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gpu-srv/PythonProject/pytorch-venv/nllo/selfsupervised-lungandheartsounds\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192bbfff-8d48-4525-9b5c-1e2b496529cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/process_heart.py --labels_only False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceed8824-851c-4ed0-adf5-da0c4be127ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart_labels.csv\n"
     ]
    }
   ],
   "source": [
    "! ls heart/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd26bed5-a47a-4171-aa15-cac5317f04a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33b4c5c-48c7-47e0-b8b6-85594507d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2359 sha256=1ee572472dc7694931698e8b27c03d83edf56174792b4a828525bc938cd658ae\n",
      "  Stored in directory: /home/gpu-srv/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post7\n"
     ]
    }
   ],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5968a2-a8be-400a-9ca5-c8ba379d1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall sklearn\n",
    "! Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2abff-09e3-4503-9896-1ab480524e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb46b414-e4ba-451d-91b6-9fc9e143fa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.0.1+cu117\n",
      "11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "#This will display the PyTorch version and the CUDA version that PyTorch was compiled with. If CUDA version is shown as \"None\", it means that PyTorch was not built with CUDA support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a05d9-c024-42df-b91c-e928b874ca05",
   "metadata": {},
   "source": [
    "python contrastive.py --mode pretrain --task heart --log_dir 8_24/spec-pre-large --data ../heart --augment spec --train_prop 1.0 --epoch 10\n",
    "Using cache found in /home/gpu-srv/.cache/torch/hub/harritaylor_torchvggish_master\n",
    "Log Dir: /home/gpu-srv/PythonProject/pytorch-venv/nllo/selfsupervised-lungandheartsounds/models/../heart/logs/8_24/spec-pre-large\n",
    "Running on: cpu\n",
    "Batch Size: 16\n",
    "/home/gpu-srv/miniconda3/envs/stanfordML/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
    "  warnings.warn(\n",
    "/home/gpu-srv/miniconda3/envs/stanfordML/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
    "  warnings.warn(msg)\n",
    "Feature extractor: resnet18\n",
    "pretrained weights not found.\n",
    "tensorboard_dir: /home/gpu-srv/PythonProject/pytorch-venv/nllo/selfsupervised-lungandheartsounds/models/../heart/logs/8_24/spec-pre-large/runs/08-24-15-47-39\n",
    "\n",
    "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7fef942df7e0>\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/gpu-srv/miniconda3/envs/stanfordML/lib/python3.11/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
    "    self._make_module_from_path(filepath)\n",
    "  File \"/home/gpu-srv/miniconda3/envs/stanfordML/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
    "    module = module_class(filepath, prefix, user_api, internal_api)\n",
    "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/gpu-srv/miniconda3/envs/stanfordML/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
    "    self.version = self.get_version()\n",
    "                   ^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/gpu-srv/miniconda3/envs/stanfordML/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
    "    config = get_config().split()\n",
    "             ^^^^^^^^^^^^^^^^^^\n",
    "AttributeError: 'NoneType' object has no attribute 'split'\n",
    "pretrain KNN Acc: 0.780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf507370-b1d1-4b23-b58d-ae428a310633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90077a48-6a1e-4efd-b5a8-d4c6ef6df8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move a tensor to the device\n",
    "\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "# Move a model to the device\n",
    "#model = MyModel()\n",
    "#model = model.to(device)\n",
    "\n",
    "# Perform computations on the device\n",
    "output = tensor + 1\n",
    "\n",
    "# Run the model on the device\n",
    "#output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658aeda1-c0dc-4e84-a25b-2c1369db74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01319e27-5f0f-4008-9755-675d52476ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.14.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir=\"heart/logs/8_25/spec_pre_large/runs/1/events.out.tfevents.1693166573.gpusrv-System-Product-Name.734664.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da4d90c-f1e9-4e61-95c9-3d0d81856632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: Could not resolve hostname d24h: Temporary failure in name resolution\n"
     ]
    }
   ],
   "source": [
    "! ssh -L 16006:10.64.236.36:6006 user@d24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9788c-ed64-4a4c-9672-77955a8b341c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanfordML",
   "language": "python",
   "name": "stanfordml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
